---
title: "Get full article text for GWAS Catalog studies"
author: "Isobel Beasley"
date: "2025-10-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Required packages 

```{r, message=FALSE}

library(httr)
library(xml2)
library(stringr)
library(here)
library(dplyr)
library(data.table)

```

# Get PMCIDs 

## Get Pubmed ids from GWAS catalog

```{r get_pmids}

# gwas_study_info <- data.table::fread(here::here("output/gwas_cat/gwas_study_info_trait_group_l2.csv"))

## Step 1: 
# get only relevant disease studies
# gwas_study_info <- data.table::fread(here::here("output/gwas_cat/gwas_study_info_trait_group_l2.csv"))
gwas_study_info <- data.table::fread(here::here("output/icd_map/gwas_study_gbd_causes.csv"))

gwas_study_info = gwas_study_info |>
  dplyr::rename_with(~ gsub(" ", "_", .x))

# filter out infectious diseases
gwas_study_info <- gwas_study_info |>
    dplyr::filter(!cause %in% c("HIV/AIDS",
                             "Tuberculosis",
                             "Malaria",
                             "Lower respiratory infections",
                             "Diarrhoeal diseases",
                             "Neonatal disorders",
                             "Tetanus",
                             "Diphtheria",
                             "Pertussis" ,
                             "Measles",
                             "Maternal disorders"))

# gwas_study_info <- gwas_study_info |>
#   dplyr::filter(DISEASE_STUDY == TRUE)

print("Number of disease studies to get full texts for:")

all_pmids <- unique(gwas_study_info$PUBMED_ID)
length(all_pmids)

```

## Convert Pubmed IDs to PMCIDs

```{r pmid_to_pmcid, warning=FALSE, eval=F}
# get PMID to PMCID mapping using Europe PMC file:
convert_pmid_df <- fread(here::here("data/europe_pmc/PMID_PMCID_DOI.csv"))

convert_pmid_df <- convert_pmid_df |>
  dplyr::rename(pmcids = PMCID
                ) |>
  dplyr::mutate(pmcids = ifelse(is.na(pmcids),
                                "",
                                pmcids
                                )
                )

convert_pmid_df <-
  convert_pmid_df |>
  dplyr::filter(!is.na(PMID))

converted_ids = 
  convert_pmid_df |>
  filter(PMID %in% all_pmids)

data.table::fwrite(converted_ids,
                   here::here("output/fulltexts/pmid_to_pmcid_mapping.csv")
                   )
```


```{r pmids_to_pmcids_stats, warning=FALSE}

converted_ids <- data.table::fread(here::here("output/fulltexts/pmid_to_pmcid_mapping.csv"))

print("Head of pmid to pmcid mapping data.frame:")
head(converted_ids)

print("Dimensions of pmid to pmcid mapping data.frame:")
dim(converted_ids)
length(all_pmids)

print("All pmids are in this data.frame, but some don't have pmcid mapping")

not_converted_pmids <-
converted_ids |>
  filter(pmcids == "")  |>
  pull(PMID)

print("Number of pmids without pmcid mapping:")
length(not_converted_pmids)

pmcids <-
converted_ids$pmcids |>
  unique()

pmcids <- pmcids[pmcids != ""]

print("Number of pmids with pmcid mapping:")
length(pmcids)

print("Percentage of pmids with pmcid:")
round(100 * length(pmcids) / length(all_pmids), digits = 2)

```

# Download full texts from European PMC

Requires PMCIDS to download full text xmls from Europe PMC Restful API. Thus, can only be applied to papers with PMCIDs.

```{r download_europe_xml, eval = F}

# Function to download full text xml from Europe PMC Restful API
download_pmc_text <- function(pmcid, 
                              out_dir = here::here("output/fulltexts/europe_pmc/")
                              ) {
  
  # check if file already exists
  if(file.exists(paste0(out_dir, pmcid, ".xml"))){
    return(TRUE)
  }


  url_xml <- paste0("https://www.ebi.ac.uk/",
                    "europepmc/webservices/rest/",
                    pmcid,
                    "/fullTextXML"
                    )
  
  resp <- GET(url_xml)
  
  # ---- Fallback URL ----
  if(status_code(resp) != 200){
    
    url_xml <- paste0("https://europepmc.org/",
                       "oai.cgi?verb=GetRecord",
                       "&metadataPrefix=pmc",
                       "&identifier=oai:europepmc.org:",
                       pmcid)
    
    resp <- GET(url_xml)
  
  }
  
  # ---- Fail if still bad ----
  if(status_code(resp) != 200){
    
  return(NULL)
    
  }
  
  # ---- Parse XML ----
  xml_content <- read_xml(
    content(resp, 
            as = "text", 
            encoding = "UTF-8")
  )
  
  article_node = xml_find_first(xml_content, 
                               "//*[local-name() = 'article']"
                               )
  
   if (is.na(article_node)) {
    message("No <article> node found for ", pmcid)
     
    return(NULL)
   }
  
  # --- Save ---
  write_xml(article_node, 
            paste0(out_dir, pmcid, ".xml")
            )
  
} 


for(article in pmcids[pmcids != ""]){

download_pmc_text(article)

}

```


```{r downloaded_european_full_texts}

euro_pmcids <-list.files(here::here("output/fulltexts/europe_pmc/"),
                  pattern = "\\.xml$")

euro_pmcids <- gsub("\\.xml$", 
       "", 
       euro_pmcids
       )

euro_pmcids <- pmcids[pmcids %in% euro_pmcids]

n_euro_pmc <- length(euro_pmcids)

print("Number of downloaded full text files from European PMC:")

print(n_euro_pmc)

print("Percentage of pmids with full text from European PMC:")
round(100 * n_euro_pmc / length(all_pmids), digits = 2)

```

# Download full texts from NCBI Cloud Service

For remaining PMCIDs / PMIDs without full text, try downloading using NCBI Cloud Service. 

```{bash download_full_texts_ncbi_cloud, eval = FALSE}

# get list of pmcids with full text - author_manuscript available
# available in XML and plain text for text mining purposes.
aws s3 cp s3://pmc-oa-opendata/author_manuscript/txt/metadata/txt/author_manuscript.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

# get list of pmcids with full text - non-commercial use
# oa_noncomm 
aws s3 cp s3://pmc-oa-opendata/oa_noncomm/txt/metadata/txt/oa_noncomm.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

# get list of pmcids with full text, commercial list
# oa_comm
aws s3 cp s3://pmc-oa-opendata/oa_comm/txt/metadata/txt/oa_comm.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

# get list of pmcids with full text, commercial list
# oa_other
aws s3 cp s3://pmc-oa-opendata/oa_other/txt/metadata/txt/oa_other.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

```

## Identify full texts already downloaded through European PMC

```{r identify_full_texts}

europeanpmc_full_texts <- 
list.files(here::here("output/fulltexts/europe_pmc"),
                  pattern = "\\.xml"
           )

# get pmcids of these files
europeanpmc_full_texts <-
  gsub("\\.xml$", 
       "", 
       europeanpmc_full_texts
       ) 

```

## Get NCBI download paths for remaining full texts (where available)

```{r get_full_text_paths}

left_over_pmcids = pmcids[!pmcids %in% europeanpmc_full_texts]

print("Number of remaining pmcids without full text:")
length(left_over_pmcids)

print("+ Number of pmids without pmcid mapping:")
length(not_converted_pmids)

author_manu = data.table::fread(here::here("output/fulltexts/aws_locations/author_manuscript.filelist.txt"))

oa_noncomm = data.table::fread(here::here("output/fulltexts/aws_locations/oa_noncomm.filelist.txt"))

oa_comm = data.table::fread(here::here("output/fulltexts/aws_locations/oa_comm.filelist.txt"))

author_manu_to_get <-
author_manu |>
  dplyr::filter(AccessionID %in% left_over_pmcids | 
                PMID %in% not_converted_pmids)

print("Number of papers to download in Author Manuscripts section:")
nrow(author_manu_to_get)

oa_noncomm_to_get = 
oa_noncomm |>
  dplyr::filter(AccessionID %in% left_over_pmcids | 
                PMID %in% not_converted_pmids)

# remove any overlaps between sections
oa_noncomm_to_get <-
  oa_noncomm_to_get |>
  dplyr::filter(!c(PMID %in% author_manu_to_get$PMID))

print("Number of additional papers to download in the Non-commericial Open Access PMC section:")
nrow(oa_noncomm_to_get)

oa_comm_to_get = 
oa_comm |>
  dplyr::filter(AccessionID %in% left_over_pmcids |
                PMID %in% not_converted_pmids)

oa_comm_to_get <-
  oa_comm_to_get |>
  dplyr::filter(!c(PMID %in% author_manu_to_get$PMID)) |>
  dplyr::filter(!c(PMID %in% oa_noncomm_to_get$PMID))

# remove any overlaps between sections
print("Number of additional papers to download in the Commercial Open Access PMC section after removing overlaps with Author Manuscripts:")
nrow(oa_comm_to_get)

file_paths = 
c(oa_noncomm_to_get$Key,
  oa_comm_to_get$Key,
  author_manu_to_get$Key)

file_paths <- str_replace_all(file_paths,
                              pattern = "txt",
                              replacement = "xml")

writeLines(
  file_paths,
  here::here("output/fulltexts/aws_locations/selected_paths.txt")
)

```

## Download remaining full texts from NCBI Cloud Service

```{r download_remaining_full_texts, eval = F}

system(
  paste(
    "xargs -I {} aws s3 cp",
    "s3://pmc-oa-opendata/{}",
    shQuote(here::here("output/fulltexts/ncbi_cloud/")),
    "--no-sign-request",
    "<",
    shQuote(here::here("output/fulltexts/aws_locations/selected_paths.txt"))
  )
)

```


```{r ncbi_full_text_stats}

# not_available = left_over_pmcids[!c(left_over_pmcids %in% 
#                                           c(oa_noncomm_to_get$AccessionID, 
#                                             oa_comm_to_get$AccessionID,
#                                             author_manu_to_get$AccessionID)
#                                           )]

# get list of pmcids already retrieved
ncbi_pmcids_retrieved <- 
list.files(c(#here::here("output/fulltexts/europe_pmc"),
             here::here("output/fulltexts/ncbi_cloud/")
             ),
             pattern = "\\.xml$"
)

ncbi_pmcids_retrieved <-
  gsub("\\.xml$", 
       "", 
       ncbi_pmcids_retrieved
       )

pmids_retrieved <-
converted_ids  |>
  filter(pmcids %in% c(ncbi_pmcids_retrieved, euro_pmcids)) |>
  pull(PMID)

not_available <- all_pmids[!c(all_pmids %in% pmids_retrieved)] 

print("Percentage of pmids with full text from NCBI Cloud Service:")
100 * (length(all_pmids) - n_euro_pmc - length(not_available)) / length(all_pmids)

print("Percentage of pmids without full text from either European PMC or NCBI Cloud Service:")
100 * length(not_available) / length(all_pmids)

```

# Download from publisher (uses dois)

## Get dois for remaining articles to get full full text

```{r get_from_publisher}

doi_information <-
converted_ids |>
  filter(PMID %in% not_available)

```

## Function to get CrossRef meta-data / link information: 

```{r, message=FALSE, warning=FALSE}

library(rcrossref)
library(httr)

# Get download links from Crossref
get_crossref_links <- function(doi) {
  
  # Query Crossref for the article
  works <- cr_works(dois = doi)
  
  # keep links for xml or text-mining 
  links <- works$data$link[[1]]
  
  if(is.null(links)){
    link_data <- data.frame(doi = doi,
                            URL = NA,
                            content.type = NA,
                            content.version = NA,
                            intended.application = NA)
    return(link_data)
  }
  
  links <- 
  links |>
    filter(intended.application == "text-mining" | content.type == "application/xml"
             ) 
  
  
  
  if(nrow(links) == 0){
    link_data <- data.frame(doi = doi,
                            URL = NA,
                            content.type = NA,
                            content.version = NA,
                            intended.application = NA)
  } else{
  
  link_data <- 
  data.frame(doi = doi,
             links)
  
  }

  return(link_data)
} 

```

## Download xml full texts from publisher links

### Download full text from Elsevier API links

- Can download full text (xmls etc.) for open access articles using Elsevier API by getting token from: https://dev.elsevier.com/
- Can download full text using this token for subscribed content (for non-commercial, academic purposes) after contacting Elsevier using the following information: https://dev.elsevier.com/api_key_settings.html
- XMLS are in Elsevier's proprietary XML format (not JATS)

```{r get_elsevier_dois}

# elsevier dois:
elsevier_doi_patterns <-	"10.1016|10.1053|10.1086|10.1194|10.1593|10.1097/jto."

elsevier_dois <- grep(elsevier_doi_patterns,
                          doi_information$DOI,
                           value = TRUE
                           )

print("Number of papers potentially can get from Elsevier:")
length(elsevier_dois)

```

```{r download_elsevier_dois, eval = F}

elsevier_api_key <- Sys.getenv("ELSEVIER_API_KEY")

elsevier_doi_info <- str_remove_all(pattern = "https://doi.org/", 
                                     string = elsevier_dois)

# get pmids for elsevier dois
pmids_elsevier <- doi_information |>
  filter(DOI %in% elsevier_dois) |>
  mutate(DOI = str_remove_all(DOI,
                                 pattern = "https://doi.org/"
                                 )
         ) |>
rename_with(~tolower(.x)) 

# get elsevier full text links from crossref
elsevier_link_df <- purrr::map(elsevier_doi_info,
                              ~get_crossref_links(.x)
                              ) |> 
  bind_rows()

print("Number of Elsevier links retrieved from Crossref:")
nrow(elsevier_link_df)

print("Number of xml links retrieved from Elsevier links:")
elsevier_link_df |>
  filter(content.type == "text/xml") |>
  nrow()

elsevier_links <- elsevier_link_df |>
                  filter(!is.na(URL)) 

elsevier_links <- elsevier_links |>
  left_join(pmids_elsevier,
            by = c("doi")
            )

# get only xml links
elsevier_links <-
elsevier_links |>
  filter(content.type == "text/xml") 


download_elsevier_text <- function(url, 
                                   api_key,
                                   pmid,
                                   out_dir = here::here("output/fulltexts/elsevier/elsevier_xml/")) {
  
  # if(file.exists(paste0(out_dir, pmid, ".xml"))|file.exists(paste0(out_dir, pmid, ".txt"))
  #    ){
  #   return(TRUE)
  # }
  
  response <- GET(url,
                  add_headers("X-ELS-APIKey" = api_key)
                  )
  
  # if (status_code(response) != 200) {
  #   message("Failed to fetch text for ", pmid)
  #   return(FALSE)
  #   
  # }
  
  ct <- headers(response)[["content-type"]]
  
  #print(ct)
  
  if(grepl("text/plain", ct)){
    message("Received plain text for ", pmid, 
            " - skipping for now."
            )
    
    return(TRUE)
    
    # text_content <- content(response, type = "text/plain")
    # 
    # writeLines(text_content,
    #            paste0(out_dir, pmid, ".txt"),
    #            useBytes = TRUE)
    
    
  } else {
  
  xml_content <- content(response, 
                         encoding = "UTF-8",
                         type = "text/xml")
  
  
  article_node <- xml2::xml_find_first(
    xml_content,
    ".//*[local-name()='originalText']"
)
  
      xml2::write_xml(article_node, 
                        file = paste0(out_dir, pmid, ".xml")
    )
      
  }
    
  # writeLines(text_content,
  #            paste0(out_dir, pmid, ".txt"),
  #            useBytes = TRUE)
  
}

purrr::walk2(elsevier_links$URL,
              elsevier_links$pmid,
                ~download_elsevier_text(url = .x,
                                        api_key = elsevier_api_key,
                                        pmid = .y)
                )

```

Convert Elsevier xmls to JATS xml files 

```{bash convert_elsevier_to_jats, eval = F}

mkdir -p output/fulltexts/elsevier/xml

for file in output/fulltexts/elsevier/elsevier_xml/*.xml; do 
    filename=$(basename "$file")
    Rscript code/full_text_conversion/elsevier_to_jats_v4.R "$file" "output/fulltexts/elsevier/xml/${filename%.xml}.xml"
done

```

```{r how_many_elsevier_texts}

print("Number of downloaded full text files (xml) from Elsevier:")
list.files(here::here("output/fulltexts/elsevier/elsevier_xml/"),
             pattern = "\\.xml$"
             ) |>
  length()

```

### Download full text from Sage 

Policies:

- Permitted for non-commercial text mining with institutional access
- https://journals.sagepub.com/page/policies/text-and-data-mining

```{r sage_full_texts, eval = F}

sage_doi_patterns <- "10.1177|10.1089"

sage_links <-
  grep(sage_doi_patterns,
       doi_information$DOI,
       value = TRUE)

sage_links <- str_remove_all(pattern = "https://doi.org/", 
                              string = sage_links)

sage_link_df <- purrr::map(sage_links, 
                      ~get_crossref_links(.x)) |> 
  bind_rows()


# then had to download manually using provided xml links
# to use institutional login details 
# http://www.liebertpub.com/doi/full-xml/10.1089/omi.2017.0019
# https://journals.sagepub.com/doi/full-xml/10.1177/00220345211051967
# https://journals.sagepub.com/doi/full-xml/10.1177/0271678X211066299
# these are in JATS .xml format

# saved to output/fulltexts/sage
length(sage_links)

```

```{r sage_full_texts_check}

print("Number of downloaded full text files (xml) from Sage:")
length(list.files(here::here("output/fulltexts/sage"),
             pattern = "\\.xml$"
)
)

```

### Download full text from Springer Nature Open Access API 

- JATS xml format

```{r download_springer_full_texts, eval = F}

springer_nature_links <-
  grep("nature|10.1038/ng|10.1007/s0|10.1007|10.1038/ejhg|10.1038/tpj|10.1038/jhg|10\\.1038/|10\\.1007/",
       doi_information$DOI,
       value = TRUE)

springer_nature_links <- str_remove_all(pattern = "https://doi.org/", 
                                        string = springer_nature_links)




pmids <- doi_information %>%
  filter(DOI %in% paste0("https://doi.org/", 
                         springer_nature_links)) %>%
  pull(PMID)

check_springer_oa <- function(doi, 
                              api_key,
                              pmids,
                              out_dir = here::here("output/fulltexts/springer_nature/")) {
  
  if(file.exists(paste0(out_dir, pmids, ".xml"))){
    return(data.frame(doi = doi, 
                      openaccess = TRUE)
    )
  }
  
  url<- paste0("https://api.springernature.com/openaccess/jats?",
               "api_key=", oa_api_key,
               "&q=", doi
  )
  
  response <- GET(url)
  
  # if the request fails, return data.frame with doi and oa = F
  if (status_code(response) != 200) {
    return(data.frame(doi = doi, 
                      openaccess = FALSE)
           )
  } else {
    
    xml_content <- content(response)
    
    article_node <- xml2::xml_find_all(xml_content, ".//records")
    
  if (xml2::xml_text(article_node) == "") {
    
    return(data.frame(doi = doi, 
                      openaccess = FALSE)
    )
    
  }
    
  }
    
    xml2::write_xml(article_node, 
              paste0(out_dir, pmids, ".xml")
    )
    
    return(data.frame(doi = doi, 
                      openaccess = TRUE)
    )
    
}
  
  oa_status <-
purrr::map2(springer_nature_links,
           pmids,
           ~check_springer_oa(doi = .x,
                              api_key = oa_api_key,
                              pmids = .y)
)
  
  
  oa_status_df <- oa_status |> bind_rows()
  
  oa_status_df |> group_by(openaccess) |>
    summarise(n = n())
  
  oa_status_df |>
    filter(openaccess == FALSE)

```

```{r springer_full_texts_check}

print("Number of downloaded full text files (xml) from Springer Nature:")
length(list.files(here::here("output/fulltexts/springer_nature"),
             pattern = "\\.xml$"
)
)

print("Number of downloaded html files from Springer Nature:")
length(list.files(here::here("output/fulltexts/springer_nature"),
                  recursive = TRUE,
             pattern = "\\.html$"
)
)


```

### Download full text from Wiley 

Wiley Text & Data-mining Policy: https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining

- Can download PDFs using their API with token
- Download xmls using https://onlinelibrary.wiley.com/doi/full-xml/[DOI]

These xmls are in Wiley's proprietary XML format, not JATS. 

```{r which_wiley_dois_to_download}

wiley_dois <- grep("10\\.1002/|10\\.1111/", 
                   doi_information$DOI, 
                   value = TRUE)

wiley_dois <- str_remove_all(wiley_dois, "https://doi.org/")

print("Number of papers potentially can get from Wiley:")
length(wiley_dois)

```


```{r download_wiley_full_texts, eval = F}


pmids_wiley_dois <- doi_information %>%
  filter(DOI %in% paste0("https://doi.org/",
                         wiley_dois)
                         ) %>%
  pull(PMID)

download_wiley_pdf<- function(doi,
                   api_key,
                   pmids,
                   output_dir = here::here("output/fulltexts/wiley/pdf/")){
  
  # check files doesn't already exist
  if(file.exists(paste0(output_dir, pmids, ".pdf"))){
    return(NULL)
  }
  
  curl_command <- paste0('curl -L -H "Wiley-TDM-Client-Token:',
                         wiley_api,
                         '" https://api.wiley.com/onlinelibrary/tdm/v1/articles/',
                         doi, 
                         ' -o ', output_dir, pmids, '.pdf'
  )
  
  print(curl_command)

system(curl_command)

}

purrr::walk2(wiley_dois,
             pmids_wiley_dois,
             ~ download_wiley_pdf(.x, wiley_api, .y)
)

# remove zero byte files - ? I think these are failed downloads as not open access
system("find output/fulltexts/wiley/pdf -type f -size 0 -delete")

# xmls downloaded manually using https://onlinelibrary.wiley.com/doi/full-xml/[DOI]
# downloaded to fulltexts/wiley/wiley_xml

```

As xml files are in Wiley format, convert JATS XML (1.1) format to be consistent with PubMed etc. 

```{bash convert_wiley_xml, eval = F}

mkdir -p output/fulltexts/wiley/xml

for file in output/fulltexts/wiley/wiley_xml/*.xml; do 
    filename=$(basename "$file")
    Rscript code/full_text_conversion/wiley_to_jats.R "$file" "output/fulltexts/wiley/xml/${filename%.xml}.xml"
done

```

```{r how_many_wiley_texts}

# how many wiley full text xml downloaded
print("Number of downloaded full text files (xml) from Wiley:")
length(list.files(here::here("output/fulltexts/wiley/xml/"), 
                  recursive = TRUE,
                  pattern = "\\.xml$"))

# how many wiley pdfs downloaded
print("Number of downloaded full text files (pdf) from Wiley:")
length(list.files(here::here("output/fulltexts/wiley/"), 
                  recursive = TRUE,
                  pattern = "\\.pdf$"))

```

## Download html full texts from publisher links

### Download full text from BMJ Journals

TDM policy: https://bmjgroup.com/text-and-data-mining-tdm-policy/

```{r download_bmj_full_texts}

bmj_doi_patterns <- "10.1136/gutjnl|10.1136/jmedgenet"

bmj_links <-
  grep(bmj_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from BMJ:")
length(bmj_links)

# download html content from webpage
# save to output/fulltexts/bmj

```

```{r bmj_full_texts_check}

print("Number of downloaded full text files (html) from BMJ:")
length(list.files(here::here("output/fulltexts/bmj"),
                  recursive = TRUE,
             pattern = "\\.html$"
)
)

```

### Download full text from Cambridge

Policies: https://www.cambridge.org/core/services/open-research/text-and-data-mining

- Can carry out TDM on any Cambridge Core content you have lawful access to
- Contact openresearch@cambridge.org about getting xml content

```{r cambridge_full_texts}

cambridge_doi_patterns <- "10.1017"

cambridge_links <-
  grep(cambridge_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from Cambridge:")
length(cambridge_links)

# obtain html content from webpage

```

```{r cambridge_full_texts_check}

print("Number of downloaded full text files (html) from Cambridge:")
length(list.files(here::here("output/fulltexts/cambridge"),
                  recursive = TRUE,
             pattern = "\\.html$"
)
)

print("Number of downloaded full text files (xml) from Cambridge:")
length(list.files(here::here("output/fulltexts/cambridge"),
                  recursive = TRUE,
             pattern = "\\.xml$"
)
)

```

### Download full text from Oxford Academic 

Oxford Academic TDM policy: https://academic.oup.com/pages/purchasing/rights-and-permissions/text-and-data-mining

*should reach out to confirm UCSF rights / possibly get xml formats 

```{r oxford_full_texts_potential}

# go doi pages, and download html manually 
oxford_dois <- grep("10.1093|10.113/amiajnl|10.1210|10.1513", 
                   doi_information$DOI, 
                   value = TRUE)

print("Number of papers potentially can get from Oxford Academic:")
length(oxford_dois)

```

```{r oxford_how_many_full_texts}

# then had to download manually using institutional login
# then saved to output/fulltexts/oxford_academic/html
oxford_htmls <- list.files(here::here("output/fulltexts/oxford_academic/html/"),
                             pattern = "\\.html$"
                             )

print("Number of downloaded full text files (html) from Oxford Academic:")
length(oxford_htmls)

```

#### Convert html to txt files

```{r download_oxford_full_texts, eval = F}

# convert html to txt

for(html_file in oxford_htmls){
  
  html_path <- here::here("output/fulltexts/oxford_academic/html/",
                          html_file
                          )
  
  html_content <- rvest::read_html(html_path)
  
  text_content <- rvest::html_text2(html_content)
  
  writeLines(text_content,
             here::here("output/fulltexts/oxford_academic/txt/",
                        gsub("\\.html$", ".txt", html_file)
                        ),
             useBytes = TRUE)
  
}

```

```{r oxford_full_texts_check}

print("Number of downloaded full text files (html) from Oxford Academic:")
length(list.files(here::here("output/fulltexts/oxford_academic/html"),
                  pattern = "\\.html$"
)
)

print("Number of downloaded full text files (txt) from Oxford Academic:")
length(list.files(here::here("output/fulltexts/oxford_academic/txt"),
                  pattern = "\\.txt$"
)
)

```

### Download full text from Taylor & Francis

TDM policy / information: https://taylorandfrancis.com/our-policies/textanddatamining/

- " If you or your institution subscribes to content from Taylor & Francis you can carry out TDM activities on this content, as well as open access content, without any additional charge, provided this is on a non-commercial basis. "

```{r download_taylor_francis_full_texts, eval = F}

taylor_francis_dois <- grep("10.1080|10.2217", 
                   doi_information$DOI,
                   value = TRUE)

print("Number of papers potentially can get from Taylor & Francis:")
length(taylor_francis_dois)

# then had to download manually using institutional login
# as html
# saved to output/fulltexts/taylor_and_francis/html

```

```{r taylor_francis_full_texts_check}

print("Number of downloaded full text files (html) from Taylor & Francis:")
length(list.files(here::here("output/fulltexts/taylor_and_francis/html"),
                  pattern = "\\.html$"
)
)

print("Number of downloaded full text files (xml) from Taylor & Francis:")
length(list.files(here::here("output/fulltexts/taylor_and_francis"),
                  recursive = TRUE,
                  pattern = "\\.xml$")
)

```

# To get text-mining license info for:

American Physiological Society, doi: 10.1152

```{r aps_full_texts}

# check, how many papers:
aps_doi_patterns <- "10.1152"

aps_links <-
  grep(aps_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from APS:")
length(aps_links)

```

American Association for Cancer Research, doi: 10.1158

```{r aacr_full_texts}

aacr_doi_patterns <- "10.1158"

aacr_links <-
  grep(aacr_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from AACR:")
length(aacr_links)

```

AHA, doi: 10.1161

```{r aha_full_texts}

aha_doi_patterns <- "10.1161"

aha_links <-
  grep(aha_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from AHA:")
length(aha_links)

```
? ATS: doi: 10.1164, 10.1165 (moving to Oxford Academic in March 2026)

```{r ats_full_texts}

ats_doi_patterns <- "10.1164|10.1165"

ats_links <-
  grep(ats_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from ATS:")
length(ats_links)

```

ASH, doi: 10.1182

```{r ash_full_texts}

ash_doi_patterns <- "10.1182"

ash_links <-
  grep(ash_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from ASH:")
length(ash_links)

```

ERS, doi: 10.1183

```{r ers_full_texts}

ers_doi_patterns <- "10.1183"

ers_links <-
  grep(ers_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from ERS:")
length(ers_links)

```


ASCO, doi: 10.1200

```{r asco_full_texts}

asco_doi_patterns <- "10.1200"

asco_links <-
  grep(asco_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from ASCO:")
length(asco_links)

```

AAN, doi: 10.1212

```{r aan_full_texts}

aan_doi_patterns <- "10.1212"

aan_links <-
  grep(aan_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from AAN:")
length(aan_links)

```

J-STAGE: doi: 10.1248

```{r jstage_full_texts}

jstage_doi_patterns <- "10.1248"

jstage_links <-
  grep(jstage_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from J-STAGE:")
length(jstage_links)

```

JASN: doi: 10.1681

```{r jasn_full_texts}

jasn_doi_patterns <- "10.1681"

jasn_links <-
  grep(jasn_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from JASN:")
length(jasn_links)

```

(ADA) Diabetes, doi: 10.2337

- https://diabetesjournals.org/journals/pages/ada-journal-policies
- ? Seems likely text-mining may be allowed, 
https://diabetesjournals.org/journals/pages/license

```{r diabetes_full_texts}

diabetes_doi_patterns <- "10.2337"

diabetes_links <-
  grep(diabetes_doi_patterns,
       doi_information$DOI,
       value = TRUE)

print("Number of papers potentially can get from Diabetes:")
length(diabetes_links)

```

# Total number of downloaded full texts

```{r final_full_text_stats}

full_text_files <-
  list.files(here::here("output/fulltexts"), 
             recursive = T, 
             pattern = "\\.html$|\\.xml$") 

full_text_files <- basename(full_text_files) |> 
  stringr::str_remove_all("\\.html$|\\.xml$") |>
  unique()

# convert pmcids to pmids 
converted_fulltext_pmcids <-
  converted_ids |>
  filter(pmcids %in% full_text_files) |>
  pull(PMID) |>
  unique() 

full_text_files  <- c(full_text_files, 
                      converted_fulltext_pmcids)

full_text_pmids <- grep("PMC", 
                        full_text_files, 
                        invert = T, 
                        value = T)

full_text_pmids = unique(full_text_pmids)

print("Number of PMIDs with full texts downloaded:")
sum(all_pmids %in% full_text_files)

print("% of total PMIDs with full texts downloaded:")
100 * sum(all_pmids %in% full_text_files) / length(all_pmids)

```

## Number of papers to manually review

The papers I can't get full texts for automatically (either through Europe PMC, NCBI Cloud Service, or publisher TDM policies) will need to be manually reviewed to identify study cohorts. 

```{r papers_to_review}

print("Number of PMIDs without full texts downloaded (to manually review):")
n_manual_review = length(all_pmids) - sum(all_pmids %in% full_text_files)
n_manual_review

print("Assuming 10 minutes per paper to review, total time (hours):")
n_manual_review * 10 / 60

```

# Testing: 

## Other open alex wiley links

```{r, eval = F}
open_alex_wiley_urls <-
open_alex_works |> 
  filter(doi %in% paste0("https://doi.org/", remaining_doi_info)) |> 
  filter(is_oa_anywhere == T) |> 
  filter(grepl("onlinelibrary.wiley.com", oa_url)) 

open_alex_wiley_dois <-
open_alex_wiley_urls |>
  pull(doi)

doi_information |>
  filter(DOI %in% open_alex_wiley_dois
         ) |>
  pull(PMID) -> pmids_open_alex_wiley

purrr::walk2(open_alex_wiley_urls$oa_url,
             pmids_open_alex_wiley,
             ~ download_wiley_pdf(doi = .x,
                                  api_key = wiley_api,
                                  pmids = .y)
             )



```

## Download PDFs using Open Access information from Open Alex

### PMIDs that couldn't be converted to PMCIDs

```{r eval=FALSE}

# old getting dois:
entrez_info <-
entrez_summary(db="pubmed", 
               id=not_convertable_pmids)

dois <-
entrez_info |>
  purrr::map(function(x) {
    
    x$articleids |> 
      filter(idtype == "doi") |> 
      pull(value)
  }
)

```

```{r open_alex_full_text, eval = F}

library(openalexR)

convert_pmid_df <- fread(here::here("data/europe_pmc/PMID_PMCID_DOI.csv"))

not_convertable_pmids <- converted_ids |> 
                         filter(pmcids == "") |> 
                         pull(PMID)

doi_information <-
convert_pmid_df |>
  filter(PMID %in% not_convertable_pmids)

doi_information |>
  filter(DOI == "")

doi_information$PMID |> unique() |> length()

length(not_convertable_pmids)

# get open alex works for pmids
open_alex_works <- oa_fetch(
  doi = unique(doi_information$DOI),
  entity = "works",
  options = list(select = c("doi", 
                            "open_access"))
)

# no best open access location: 
open_alex_works |> 
  filter(is.na(oa_url)) |>
  nrow()

# pdf link available:
open_alex_works |> 
  filter(grepl("pdf", oa_url)) |>
  nrow()

to_download_pdfs <-
open_alex_works |> 
  filter(grepl(".pdf", oa_url)) |>
  pull(oa_url)

  writeLines(
    to_download_pdfs,
    here::here("output/fulltexts/pdfs/pdf_links_to_download.txt"))

```

```{bash download_pdfs_1, eval = F}

cd output/fulltexts/pdfs

while read -r url; do
  curl -O "$url"
done < pdf_links_to_download.txt

```

### PMCIDs not found in Author Manuscripts or Open Access sections

```{r, eval=FALSE}

doi_information <-
convert_pmid_df |>
  filter(PMCID %in% not_available)

doi_information |>
  filter(DOI == "")

doi_information <-
  doi_information |>
  filter(DOI != "")

# get open alex works for pmcids
open_alex_works <- oa_fetch(
  doi = doi_information$DOI,
  entity = "works",
  options = list(select = c(#"title",
                            "doi", 
                            "open_access"
                            ))
)

# no best open access location: 
open_alex_works |> 
  filter(is.na(oa_url)) |>
  nrow()

# pdf link available:
open_alex_works |> 
  filter(grepl("pdf", oa_url)) |>
  nrow()

to_download_pdfs <-
open_alex_works |> 
  filter(grepl(".pdf", oa_url)) |>
  pull(oa_url)

  writeLines(
    to_download_pdfs,
    here::here("output/fulltexts/pdfs/pdf_links_to_download_pt2.txt"))

```

```{bash download_pdfs_2, eval = F}

cd output/fulltexts/pdfs

while read -r url; do
  curl -O "$url"
done < pdf_links_to_download_pt2.txt

```

## Test: not used - Europe PMC Author Manuscripts

```{bash get_filelists, eval = FALSE}

curl -s https://europepmc.org/ftp/manuscripts/ \
  | grep -o 'author_manuscript_txt[^"]*\.filelist\.txt' \
  | sort -u \
  | while read -r file; do
      curl -O "https://europepmc.org/ftp/manuscripts/$file"
    done

```

```{r europe_pmc_author_manuscripts, eval = F}

all_file_lists <- list.files(here::here("data/epmc"))

author_manu_epmc <- all_file_lists |>
                    purrr::map(function(file_name) {
                      
                      file_path = here::here("data/epmc",
                                             file_name
                                             )
                      
                      df <- fread(file_path)
                      
                      return(df)
                    }
                    ) |>
                    bind_rows()

author_manu_epmc |>
  filter(AccessionID %in% not_available)

author_manu_epmc |>
  filter(PMID %in% not_available_pmids)

author_manu_epmc |>
  filter(PMID %in% pmids)

author_manu_epmc |>
  filter(PMID %in% not_convertable_pmids)

```

## Test: not used - Download with ftp service, where available

```{r download_ftp_service, eval = F}

# not_available
not_convertable_pmids <- converted_ids |> 
                         filter(pmcids == "") |> 
                         pull(PMID)

all_tgz_links = c()

for(article_id in "PMC2613843"){

url <- paste0("https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id=",
              article_id)
  
resp <- GET(url)

xml_data <- xml_child(content(resp), "records")

tgz_link <- xml_find_first(xml_data, 
                           ".//link[@format='tgz']/@href")
tgz_link <- xml_text(tgz_link)

if (is.na(tgz_link)) {
  
  print("No tar.gz link found.")
  
} else {
  
  all_tgz_links <- append(all_tgz_links, 
                          tgz_link)
  
}
}

```
