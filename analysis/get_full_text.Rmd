---
title: "Get full article text for GWAS Catalog studies"
author: "Isobel Beasley"
date: "2025-10-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Required packages 

```{r, message=FALSE}

library(httr)
library(xml2)
library(stringr)
library(here)
library(dplyr)

```

# Get PMCIDs 

## Get Pubmed ids from GWAS catalog

```{r get_pmids}


gwas_study_info <- data.table::fread(here::here("output/gwas_cat/gwas_study_info_trait_group_l2.csv"))

gwas_study_info = gwas_study_info |>
  dplyr::rename_with(~ gsub(" ", "_", .x))

gwas_study_info <- gwas_study_info |>
  filter(DISEASE_STUDY == TRUE)

pmids <- unique(gwas_study_info$PUBMED_ID)

length(pmids)

```


## Convert Pubmed IDs to PMCIDs

```{r pmid_to_pmcid}


# convert PMID to PMCID
convert_pmid_to_pmcid <- function(pmid_vec,
                                  tool = "myTool",
                                  email = "you@example.com",
                                  format = "json",
                                  batch_size = 100,
                                  sleep_time = 1) {

  base_url <- "https://pmc.ncbi.nlm.nih.gov/tools/idconv/api/v1/articles/"

  batches <- split(pmid_vec,
                   ceiling(seq_along(pmid_vec) / batch_size))
  #browser()

  pmcid_list = purrr::map(batches,
             function(pmid_vec) {


               ids_param <- paste(pmid_vec,
                                  collapse = ",")

               query <- list(ids = ids_param,
                             idtype = "pmid",
                             tool = tool,
                             email = email,
                             format = format
                             )

               resp <- httr::GET(base_url,
                                 query = query)

               httr::stop_for_status(resp)

               content_text <- httr::content(resp,
                                             as = "text",
                                             encoding = "UTF-8")

               # Handle cases where records might be empty or missing pmcid
               parsed <- jsonlite::fromJSON(content_text,
                                            flatten = TRUE)

               parsed$records[is.na(parsed$records)] = ""

               pmcid <- parsed$records |>
                        pull(pmcid)

               Sys.sleep(sleep_time)

               return(pmcid)
             }
  )

 pmcid_list = unlist(pmcid_list)
 names(pmcid_list) <- pmid_vec
 return(pmcid_list)
}

pmcids <- convert_pmid_to_pmcid(pmids)

get_pmcid_europepmc <- function(pmid_vec) {
  base_url <- "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
  
  purrr::map_dfr(pmid_vec, function(pmid) {
    query <- list(
      query = paste0("ext_id:", pmid),
      format = "json"
    )
    resp <- httr::GET(base_url, query = query)
    if (httr::status_code(resp) != 200) {
      return(tibble(pmid = pmid, 
                    pmcid = NA_character_))
    }
    
    dat <- jsonlite::fromJSON(httr::content(resp, 
                                            as = "text", 
                                            encoding = "UTF-8"))
    
    if (length(dat$resultList$result) == 0) {
      return(tibble(pmid = pmid, pmcid = NA_character_))
    }
    
    pmcid <- dat$resultList$result$pmcid
    tibble(pmid = pmid, pmcid = pmcid)
  })
}

pmids_missing = names(pmcids[pmcids == ""])

get_pmcid_europepmc(pmids_missing) -> pmcid_europepmc_df

converted_ids <-
data.frame(pmids = names(pmcids),
           pmcids = pmcids
           ) 

data.table::fwrite(converted_ids,
                   here::here("output/gwas_cat/gwas_pubmed_to_pmcid_mapping.csv")
                   )

# How many missing? 
sum(pmcids == "")

```

# Download + clean full texts from European PMC

```{r download_full_texts}

converted_ids <-
data.table::fread(here::here("output/gwas_cat/gwas_pubmed_to_pmcid_mapping.csv")
)


download_pmc_text <- function(pmcid, 
                              out_dir = here::here("output/fulltexts")
                              ) {


  url_xml <- paste0("https://www.ebi.ac.uk/europepmc/webservices/rest/",
                    pmcid,
                    "/fullTextXML"
                    )

  resp <- GET(url_xml)

  if (status_code(resp) != 200) stop("Failed to fetch XML for ", pmcid)

  xml_content <- read_xml(content(resp,
                                  as = "text",
                                  encoding = "UTF-8")
                          )

  # Get text body xml content
  #browser()
  xml_body = xml_child(xml_content, "body")
  
  # Build text file
  # By converting xml structure into sections and subsections
  text = c()
  for(section in 1:xml_length(xml_body)){

    section_node = xml_child(xml_body, section)

    if(sum(
            xml_length(xml_find_all(section_node,
                                    ".//*[.//title and .//p]"))
            ) == 0
       )
      {

      # Get section name:
      section_name = xml_text(xml_find_all(section_node, ".//title"))
      section_name = str_squish(section_name)
      if(!rlang::is_empty(section_name)) {
         text = c(text, paste0("\n\n", section_name, "\n"))
      }

      # Get paragraphs
      para_nodes = xml_find_all(section_node, ".//p")
      para_texts = xml_text(para_nodes)
      para_texts = str_squish(para_texts)
      
      
      if(!rlang::is_empty(para_texts)) {
        text = c(text, paste0("\n", para_texts, "\n"))
      }
      
      if(rlang::is_empty(section_name) && 
         rlang::is_empty(para_texts)) {
        
        all_node_text <- xml_text(section_node)
        
        if(!rlang::is_empty(all_node_text)){
        
        text = c(text, paste0("\n", all_node_text, "\n"))
        
        }
      }
      
      label <- xml_text(xml_find_all(section_node, 
                                     ".//label"))
      href <- xml_attr(xml_find_all(section_node, 
                                    ".//media"), 
                       "href")
      
      if(!rlang::is_empty(label) && !rlang::is_empty(href)) {
        
        text = c(text, paste0("\n", label, ". ", href, "\n"))
        
      }
      
      

    } else {

    for(subsection in 1:xml_length(section_node)){

    subsection_node = xml_child(section_node,
                                subsection
                                )

    # Get section name:
    subsection_name = xml_text(xml_find_all(subsection_node, ".//title"))
    subsection_name = str_squish(subsection_name)
    if(!rlang::is_empty(subsection_name)) {
    # Add spaces around section titles
    text = c(text, paste0("\n\n", subsection_name, "\n"))
    }

    # Get paragraphs
    para_nodes = xml_find_all(subsection_node, ".//p")
    para_texts = xml_text(para_nodes)
    para_texts = str_squish(para_texts)
    
    if(!rlang::is_empty(para_texts)) {
    # Add spaces around paragraphs
    text = c(text, paste0("\n", para_texts, "\n"))
    }
    
    if(rlang::is_empty(subsection_name) &&
         rlang::is_empty(para_texts)) {    
    
        all_node_text <- xml_text(section_node)
        
        if(!rlang::is_empty(all_node_text)){
        
        text = c(text, paste0("\n", all_node_text, "\n"))
        
        }
      }
            
    }
}
  }
  
  # if Nat Genet article
  if(grepl("Nature genetics", 
        xml_text(xml_find_all(xml_content, 
                              ".//journal-title")),
        ignore.case = TRUE
        )
  ){
    
    # Find all figure nodes
    xml_figures <- xml_find_all(xml_content,
                                     ".//fig")
    
    if(sum(xml_length(xml_figures)) != 0){
     
      text = c(text, "\n\nFigures:\n")
      
    }
    
    for(nodes in 1:length(xml_figures)){
      
      figure_node = xml_figures[nodes]
      
      # Extract label:
      label <- xml_text(xml_find_all(figure_node, 
                                     ".//label"))
      
      # Extract title
      title = xml_text(xml_find_all(figure_node, ".//title"))
      
      if(!rlang::is_empty(label) | !rlang::is_empty(title)){
        
        text = c(text,
                 paste0("\n", label, ". ", title, "\n")
                 )
      }
      
      
      # Extract caption
      caption = xml_text(xml_find_all(figure_node, 
                                      ".//caption//p"))
      
      if(!rlang::is_empty(caption)){
        
        text = c(text,
                 paste0("\n", caption, "\n")
                 )
      }
      
    }
    
    # Find all tables
    xml_tables <- xml_find_all(xml_content,
                               ".//table-wrap")
    
    if(sum(xml_length(xml_tables)) != 0){
      
      text = c(text, "\n\nTables:\n")
      
    }
    
    for(nodes in 1:length(xml_tables)){
      
      table_node = xml_tables[nodes]
      
      # Extract label:
      label <- xml_text(xml_find_all(table_node, 
                                     ".//label"))
      
       if(!rlang::is_empty(label)){
        
        text = c(text,
                 paste0("\n",label, "\n")
                 )
      }
      
      # Extract caption
      caption = xml_text(xml_find_all(table_node, 
                                      ".//caption//p"))
      
      if(!rlang::is_empty(caption)){
        
        text = c(text,
                 paste0("\n", 
                        caption, "\n")
        )
      }
      
    }
    
    
    
  }

  # --- Save ---
  text_full <- paste(text, collapse = " ")

  txt_file <- file.path(out_dir,
                        paste0(pmcid, ".txt")
                        )
  writeLines(text_full,
             txt_file,
             useBytes = TRUE)

  #message("✅ Cleaned text saved for ", pmcid)
  invisible(text_full)
}


safe_download_pmc_text <- purrr::safely(download_pmc_text)


# Download full texts for all PMCIDs
for(pmcid in pmcids){
  if(pmcid != ""){
    
    result <- safe_download_pmc_text(pmcid)

    # if(!is.null(result$error)){
    #   message("❌ Failed to download text for ", pmcid,
    #           ": ", result$error)
    # }
  }
}

# How many texts saved? 
length(list.files(here::here("output/fulltexts"),
                  pattern = "\\.txt$"))

```

## For remaining PMCIDs / PMIDs without full text, try downloading using NCBI Cloud Service

```{bash download_full_texts_ncbi_cloud, eval = FALSE}

# get list of pmcids with full text - author_manuscript available
# available in XML and plain text for text mining purposes.
aws s3 cp s3://pmc-oa-opendata/author_manuscript/txt/metadata/txt/author_manuscript.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

# get list of pmcids with full text - non-commerical use
# oa_noncomm 
aws s3 cp s3://pmc-oa-opendata/oa_noncomm/txt/metadata/txt/oa_noncomm.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

```

## Identify full texts downloaded

```{r identify_full_texts}

full_texts = 
list.files(here::here("output/fulltexts"),
                  pattern = "\\.txt$")

# get pmcids of these files
gsub("\\.txt$", "", full_texts) -> full_texts

```

## Get paths of full texts could download: 

```{r get_full_text_paths}

left_over_pmcids = pmcids[!pmcids %in% full_texts]

length(left_over_pmcids)

author_manu = data.table::fread(here::here("output/fulltexts/aws_locations/author_manuscript.filelist.txt"))
oa_noncomm = data.table::fread(here::here("output/fulltexts/aws_locations/oa_noncomm.filelist.txt"))

author_manu_to_get <-
author_manu |>
  filter(PMID %in% names(left_over_pmcids))

nrow(author_manu_to_get)

oa_noncomm_to_get = 
oa_noncomm |>
  filter(PMID %in% names(left_over_pmcids)) 

nrow(oa_noncomm_to_get)

not_avaliable = names(left_over_pmcids)[!names(left_over_pmcids) %in% 
                                          c(oa_noncomm_to_get$PMID, 
                                            author_manu_to_get$PMID)]

length(not_avaliable)

file_paths = 
c(oa_noncomm_to_get$Key,
  author_manu_to_get$Key)

# percentage not avaliable, from all:
100 * length(not_avaliable) / length(pmcids)

```

## Download remaining full texts from NCBI Cloud Service

```{r download_remaining_full_texts}

# download all the available ones:
for (i in seq_along(file_paths)) {
  
  system(
  paste(
  "aws s3 cp",
  paste0("s3://pmc-oa-opendata/", file_paths[i]),
  here::here("output/fulltexts/"),
  "--no-sign-request"
  )
  )
}

```

# Get dbGaP ids / EGA sentences 

```{r get_dbgap_ids}

library(tokenizers)

get_grep_sentences <- function(pmcid,
                               in_dir = here::here("output/fulltexts"),
                               grep_pattern) {

  txt_in <- readLines(paste0(in_dir, "/", pmcid, ".txt"),
                      warn = FALSE,
                      encoding = "UTF-8")

  sentences <- unlist(tokenize_sentences(txt_in))

  dbgap_sentences = sentences[grepl(grep_pattern, sentences)]

  return(dbgap_sentences)
}

grep_pattern <- "phs\\d+|EGAC\\d+|EGAD\\d+|EGAF\\d+|JGAS\\d+" 

pmcid_dbgap_sentences <- 
purrr::map(full_texts,
           ~get_grep_sentences(.x, grep_pattern = grep_pattern)
           ) 

names(pmcid_dbgap_sentences) <- full_texts

keep_pmcid_dbgap_sentences <-
  purrr::discard(
pmcid_dbgap_sentences,
~rlang::is_empty(.x)
) 

# Convert to data frame
sentences_df <- 
  purrr::imap(keep_pmcid_dbgap_sentences,
              ~data.frame(pmcid = .y,
                          sentence = .x,
                        stringsAsFactors = FALSE)
              )|>
  dplyr::bind_rows()

# Extract all dbGaP IDs from the sentences
sentences_df <- sentences_df |>
  mutate(
    dbgap_id = str_extract_all(sentence, "phs\\d+(\\.v\\d+)?(\\.p\\d+)?"),
    ega_id = str_extract_all(sentence, "EGA[CDFS]\\d+"),
    jgas_id = str_extract_all(sentence, "JGAS\\d+")
  )
             
# number of pubmed ids with dbgap / ega ids found
sentences_df$pmcid |> unique() |> length()

data.table::fwrite(sentences_df,
                   here::here("output/gwas_cat/gwas_study_dbgap_ega_sentences.csv")
                   )

```
