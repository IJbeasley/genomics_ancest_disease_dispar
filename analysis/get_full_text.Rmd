---
title: "Get full article text for GWAS Catalog studies"
author: "Isobel Beasley"
date: "2025-10-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Required packages 

```{r, message=FALSE}

library(httr)
library(xml2)
library(stringr)
library(here)
library(dplyr)
library(data.table)

```

# Get PMCIDs 

## Get Pubmed ids from GWAS catalog

```{r get_pmids}

# gwas_study_info <- data.table::fread(here::here("output/gwas_cat/gwas_study_info_trait_group_l2.csv"))

## Step 1: 
# get only relevant disease studies
# gwas_study_info <- data.table::fread(here::here("output/gwas_cat/gwas_study_info_trait_group_l2.csv"))
gwas_study_info <- data.table::fread(here::here("output/icd_map/gwas_study_gbd_causes.csv"))

gwas_study_info = gwas_study_info |>
  dplyr::rename_with(~ gsub(" ", "_", .x))

# gwas_study_info <- gwas_study_info |>
#   dplyr::filter(DISEASE_STUDY == TRUE)

pmids <- unique(gwas_study_info$PUBMED_ID)

#pmids <- sample(pmids, size = 500)

length(pmids)

```

## Convert Pubmed IDs to PMCIDs

```{r pmid_to_pmcid}
# get PMID to PMCID mapping using Europe PMC file:
convert_pmid_df <- fread(here::here("data/europe_pmc/PMID_PMCID_DOI.csv"))

convert_pmid_df <- convert_pmid_df |>
  dplyr::rename(pmcids = PMCID
                ) |>
  dplyr::mutate(pmcids = ifelse(is.na(pmcids),
                                "",
                                pmcids
                                )
                )

convert_pmid_df =
  convert_pmid_df |>
  select(-DOI)

convert_pmid_df <-
  convert_pmid_df |>
  dplyr::filter(!is.na(PMID))

converted_ids = 
  convert_pmid_df |>
  filter(PMID %in% pmids)

dim(converted_ids)
length(pmids)

converted_ids |>
  filter(pmcids == "") |>
  dim()

# convert PMID to PMCID
# convert_pmid_to_pmcid <- function(pmid_vec,
#                                   tool = "myTool",
#                                   email = "you@example.com",
#                                   format = "json",
#                                   batch_size = 50,
#                                   sleep_time = 1) {
# 
#   base_url <- "https://pmc.ncbi.nlm.nih.gov/tools/idconv/api/v1/articles/"
# 
#   batches <- split(pmid_vec,
#                    ceiling(seq_along(pmid_vec) / batch_size))
#   #browser()
# 
#   pmcid_list = purrr::map(batches,
#              function(pmid_vec) {
# 
# 
#                ids_param <- paste(pmid_vec,
#                                   collapse = ",")
# 
#                query <- list(ids = ids_param,
#                              idtype = "pmid",
#                              tool = tool,
#                              email = email,
#                              format = format
#                              )
# 
#                resp <- httr::GET(base_url,
#                                  query = query)
# 
#                httr::stop_for_status(resp)
# 
#                content_text <- httr::content(resp,
#                                              as = "text",
#                                              encoding = "UTF-8")
# 
#                # Handle cases where records might be empty or missing pmcid
#                parsed <- jsonlite::fromJSON(content_text,
#                                             flatten = TRUE)
# 
#                parsed$records[is.na(parsed$records)] = ""
# 
#                pmcid <- parsed$records |>
#                         pull(pmcid)
# 
#                Sys.sleep(sleep_time)
# 
#                return(pmcid)
#              }
#   )
# 
#  pmcid_list = unlist(pmcid_list)
#  names(pmcid_list) <- pmid_vec
#  return(pmcid_list)
# }
# 
# pmcids <- convert_pmid_to_pmcid(pmids)
# 
# get_pmcid_europepmc <- function(pmid_vec) {
#   base_url <- "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
#   
#   purrr::map_dfr(pmid_vec, function(pmid) {
#     query <- list(
#       query = paste0("ext_id:", pmid),
#       format = "json"
#     )
#     resp <- httr::GET(base_url, query = query)
#     if (httr::status_code(resp) != 200) {
#       return(tibble(pmid = pmid, 
#                     pmcid = NA_character_))
#     }
#     
#     dat <- jsonlite::fromJSON(httr::content(resp, 
#                                             as = "text", 
#                                             encoding = "UTF-8"))
#     
#     if (length(dat$resultList$result) == 0) {
#       return(tibble(pmid = pmid, pmcid = NA_character_))
#     }
#     
#     pmcid <- dat$resultList$result$pmcid
#     tibble(pmid = pmid, pmcid = pmcid)
#   })
# }
# 
# pmids_missing = names(pmcids[pmcids == ""])
# 
# get_pmcid_europepmc(pmids_missing) -> pmcid_europepmc_df
# 
# converted_ids <-
# data.frame(pmids = names(pmcids),
#            pmcids = pmcids
#            ) 
# 
# # checked using pmids to pmcs conversion from europe pmc webservice
# # can map 38367033 -> PMC12560237
# converted_ids <- converted_ids |>
#   mutate(pmcids = ifelse(pmids == "38367033", 
#                          "PMC12560237", 
#                          pmcids
#                          )
#          )
# 
# data.table::fwrite(converted_ids,
#                    here::here("output/gwas_cat/gwas_pubmed_to_pmcid_mapping.csv")
#                    )
# 
# # How many missing? 
# sum(pmcids == "")
# 
# pmcids <- pmcids[pmcids != ""]

```

# Download full texts from European PMC

```{r get_pmcids}

pmcids <-
converted_ids$pmcids |>
  unique()

pmcids <- pmcids[pmcids != ""]

length(pmcids)

print("Percentage of pmids with pmcid:")
round(100 * length(pmcids) / length(pmids), digits = 2)

```

```{r download_xml, eval = F}

download_pmc_text <- function(pmcid, 
                              out_dir = here::here("output/fulltexts/europe_pmc/")
                              ) {


  url_xml <- paste0("https://www.ebi.ac.uk/",
                    "europepmc/webservices/rest/",
                    pmcid,
                    "/fullTextXML"
                    )
  
  resp <- GET(url_xml)
  
  # MED/20708005
  
  # ---- Fallback URL ----
  if(status_code(resp) != 200){
    
    #print(paste0("Trying alternative URL for ", pmcid))
    
    url_xml <- paste0("https://europepmc.org/",
                       "oai.cgi?verb=GetRecord",
                       "&metadataPrefix=pmc",
                       "&identifier=oai:europepmc.org:",
                       pmcid)
    
    resp <- GET(url_xml)
  
  }
  
  # ---- Fail if still bad ----
  if(status_code(resp) != 200){
    
  #message("Failed to fetch XML for ", pmcid)
  
  return(NULL)
    
  }
  
  # ---- Parse XML ----
  xml_content <- read_xml(
    content(resp, 
            as = "text", 
            encoding = "UTF-8")
  )
  
  xml_content <- read_xml(content(resp,
                                  as = "text",
                                  encoding = "UTF-8")
                          )
  
  article_node = xml_find_first(xml_content, 
                               "//*[local-name() = 'article']"
                               )
  
   if (is.na(article_node)) {
    message("No <article> node found for ", pmcid)
     
    return(NULL)
   }
  
    # --- Save ---
  write_xml(article_node, 
            paste0(out_dir, pmcid, ".xml")
            )
  
} 


for(article in pmcids[pmcids != ""]){

download_pmc_text(article)

}

```


```{r downloaded_european_full_texts}

print("Number of downloaded full text files")
print("From European PMC:")

n_euro_pmc <- length(list.files(here::here("output/fulltexts/europe_pmc/"),
                  pattern = "\\.xml$")
       )

print("Percentage of pmids with full text from European PMC:")
round(100 * n_euro_pmc / length(pmids), digits = 2)

```

```{r download_full_texts, eval = F}

converted_ids <-
data.table::fread(here::here("output/gwas_cat/gwas_pubmed_to_pmcid_mapping.csv")
)

convert_xml_text <- function(xml_content,
                             text #output text file
                             ){
  
    for(section in 1:xml_length(xml_content)){

    section_node = xml_child(xml_content, section)

    if(length(xml_path(xml_find_all(section_node,
                                    ".//*[.//title and .//p]"
                                    )
                       )
            ) == 0
       )
      {

      # Get section name:
      section_name = xml_text(xml_find_all(section_node, 
                                           ".//title"))
      
      section_name = str_squish(section_name)
      
      if(!rlang::is_empty(section_name)) {
         text = c(text, paste0("\n\n", section_name, "\n"))
      }

      # Get paragraphs
      para_nodes = xml_find_all(section_node, ".//p")
      para_texts = xml_text(para_nodes)
      para_texts = str_squish(para_texts)
      
      
      if(!rlang::is_empty(para_texts)) {
        text = c(text, paste0("\n", 
                              para_texts, 
                              "\n")
                 )
      }
      
      if(rlang::is_empty(section_name) && 
         rlang::is_empty(para_texts)) {
        
        all_node_text <- xml_text(section_node)
        
        if(!rlang::is_empty(all_node_text)){
        
        text = c(text, paste0("\n", 
                              all_node_text, 
                              "\n")
                 )
        
        }
      }
      
      label <- xml_text(xml_find_all(section_node, 
                                     ".//label"))
      
      href <- xml_attr(xml_find_all(section_node, 
                                    ".//media"), 
                       "href")
      
      if(!rlang::is_empty(label) && !rlang::is_empty(href)) {
        
        text = c(text, paste0("\n", label, ". ", href, "\n"))
        
      }
      
      

    } else {

    for(subsection in 1:xml_length(section_node)){

    subsection_node = xml_child(section_node,
                                subsection
                                )
    
    if(length(xml_children(subsection_node)) == 0){
      
      if(xml_name(subsection_node) == "title"){
        
        text = c(text, 
               paste0("\n\n", 
                      xml_text(subsection_node), 
                      "\n")
               )
        
      } else {
      
      text = c(text, 
               paste0("\n", 
                      xml_text(subsection_node), 
                      "\n")
               )
      
      }
      
      next
      
    }
                

    # Get section name:
    subsection_name = xml_text(xml_find_all(subsection_node, 
                                            ".//title")
                               )
    
    subsection_name = str_squish(subsection_name)
    
    if(!rlang::is_empty(subsection_name)) {
    # Add spaces around section titles
    text = c(text, paste0("\n\n", 
                          subsection_name, 
                          "\n")
             )
    }

    # Get paragraphs
    para_nodes = xml_find_all(subsection_node, 
                              ".//p")
    
    para_texts = xml_text(para_nodes)
    para_texts = str_squish(para_texts)
    
    if(!rlang::is_empty(para_texts)) {
    # Add spaces around paragraphs
    text = c(text, paste0("\n", 
                          para_texts, 
                          "\n")
             )
    }
    
    if(rlang::is_empty(subsection_name) &&
         rlang::is_empty(para_texts)) {    
    
        all_node_text <- xml_text(subsection_node)
        
        if(!rlang::is_empty(all_node_text)){
        
        text = c(text, paste0("\n", 
                              all_node_text, 
                              "\n")
                 )
        
        }
      }
            
    }
}
  }
  
  return(text)
  
}

extract_app_text <- function(xml_back, 
                             text){
  
  # Add separator for appendices section
  text <- c(text, "\n\n=== APPENDICES ===\n")
  
  #browser()
  
  for(node_id in 1:xml_length(xml_back)){
    
    node = xml_child(xml_back, 
                     node_id)
    
    #print(node)
    
    if(xml_name(node) == "app-group" & 
       length(xml_find_all(node, ".//sec")) > 0) {
      
      app_node = xml_find_all(node, ".//sec")
      
      text = convert_xml_text(app_node, text)
      
    } else if(xml_name(node) == "ref-list") {
      
      next 
      
    } else {
      
      text = convert_xml_text(node, 
                              text)
    }
  }


download_pmc_text <- function(pmcid, 
                              out_dir = here::here("output/fulltexts/europepmc")
                              ) {


  url_xml <- paste0("https://www.ebi.ac.uk/europepmc/webservices/rest/",
                    pmcid,
                    "/fullTextXML"
                    )

  resp <- GET(url_xml)

  if (status_code(resp) != 200) stop("Failed to fetch XML for ", pmcid)

  xml_content <- read_xml(content(resp,
                                  as = "text",
                                  encoding = "UTF-8")
                          )

  # Get text body xml content
  #browser()
  xml_body = xml_child(xml_content, "body")
  xml_back = xml_child(xml_content, "back")
  #browser()
  
  # Build text file
  # By converting xml structure into sections and subsections
  text = c()
  text = convert_xml_text(xml_body,
                          text
                          )
  
  text = c(text, "\n\n")
  text = extract_app_text(xml_back,
                          text
                          )
  
    # if Nat Genet article
  if(grepl("Nature genetics", 
        xml_text(xml_find_all(xml_content, 
                              ".//journal-title")),
        ignore.case = TRUE
        )
  ){
    
    # Find all figure nodes
    xml_figures <- xml_find_all(xml_content,
                                     ".//fig")
    
    if (length(xml_figures) != 0){
     
      text = c(text, "\n\nFigures:\n")
      
    }
    
    for(nodes in 1:length(xml_figures)){
      
      figure_node = xml_figures[nodes]
      
      # Extract label:
      label <- xml_text(xml_find_all(figure_node, 
                                     ".//label"))
      
      # Extract title
      title = xml_text(xml_find_all(figure_node, ".//title"))
      
      if(!rlang::is_empty(label) | !rlang::is_empty(title)){
        
        text = c(text,
                 paste0("\n", label, ". ", title, "\n")
                 )
      }
      
      # Extract caption
      caption = xml_text(xml_find_all(figure_node, 
                                      ".//caption//p"))
      
      if(!rlang::is_empty(caption)){
        
        text = c(text,
                 paste0("\n", caption, "\n")
                 )
      }
      
    }
    
    }
  
  
  # --- Save ---
  text_full <- paste(text, collapse = " ")

  txt_file <- file.path(out_dir,
                        paste0(pmcid, ".txt")
                        )
  writeLines(text_full,
             txt_file,
             useBytes = TRUE)

  #message("✅ Cleaned text saved for ", pmcid)
  invisible(text_full)
}


safe_download_pmc_text <- purrr::safely(download_pmc_text)


# Download full texts for all PMCIDs
for(pmcid in pmcids){
  if(pmcid != ""){
    
    result <- safe_download_pmc_text(pmcid)

    # if(!is.null(result$error)){
    #   message("❌ Failed to download text for ", pmcid,
    #           ": ", result$error)
    # }
  }
}

# How many texts saved? 
length(list.files(here::here("output/fulltexts/europepmc"),
                  pattern = "\\.txt$")
       )

```

## For remaining PMCIDs / PMIDs without full text, try downloading using NCBI Cloud Service

```{bash download_full_texts_ncbi_cloud, eval = FALSE}

# get list of pmcids with full text - author_manuscript available
# available in XML and plain text for text mining purposes.
aws s3 cp s3://pmc-oa-opendata/author_manuscript/txt/metadata/txt/author_manuscript.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

# get list of pmcids with full text - non-commercial use
# oa_noncomm 
aws s3 cp s3://pmc-oa-opendata/oa_noncomm/txt/metadata/txt/oa_noncomm.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

# get list of pmcids with full text, commercial list
# oa_comm
aws s3 cp s3://pmc-oa-opendata/oa_comm/txt/metadata/txt/oa_comm.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

# get list of pmcids with full text, commercial list
# oa_other
aws s3 cp s3://pmc-oa-opendata/oa_other/txt/metadata/txt/oa_other.filelist.txt output/fulltexts/aws_locations/  --no-sign-request 

```

## Identify full texts downloaded

```{r identify_full_texts}

europeanpmc_full_texts <- 
list.files(here::here("output/fulltexts/europe_pmc"),
                  pattern = "\\.xml"
           )

# get pmcids of these files
europeanpmc_full_texts <-
  gsub("\\.xml$", 
       "", 
       europeanpmc_full_texts
       ) 

```

## Get paths of full texts could download: 

```{r get_full_text_paths}

left_over_pmcids = pmcids[!pmcids %in% europeanpmc_full_texts]

print("Number of remaining pmcids without full text:")
length(left_over_pmcids)

author_manu = data.table::fread(here::here("output/fulltexts/aws_locations/author_manuscript.filelist.txt"))

oa_noncomm = data.table::fread(here::here("output/fulltexts/aws_locations/oa_noncomm.filelist.txt"))

oa_comm = data.table::fread(here::here("output/fulltexts/aws_locations/oa_comm.filelist.txt"))

author_manu_to_get <-
author_manu |>
  dplyr::filter(AccessionID %in% left_over_pmcids)

print("Number of papers to download in Author Manuscripts section:")
nrow(author_manu_to_get)

oa_noncomm_to_get = 
oa_noncomm |>
#  dplyr::filter(PMID %in% names(left_over_pmcids)) 
  dplyr::filter(AccessionID %in% left_over_pmcids)

oa_comm_to_get = 
oa_comm |>
#  dplyr::filter(PMID %in% names(left_over_pmcids)) 
  dplyr::filter(AccessionID %in% left_over_pmcids)

print("Number of papers to download in Open Access PMC section:")
nrow(oa_noncomm_to_get) + nrow(oa_comm_to_get)

oa_noncomm_to_get <-
  oa_noncomm_to_get |>
  dplyr::filter(!c(AccessionID %in% author_manu_to_get$AccessionID))

oa_ncomm_to_get <-
  oa_noncomm_to_get |>
  dplyr::filter(!c(AccessionID %in% author_manu_to_get$AccessionID))

not_available = left_over_pmcids[!c(left_over_pmcids %in% 
                                          c(oa_noncomm_to_get$AccessionID, 
                                            oa_comm_to_get$AccessionID,
                                            author_manu_to_get$AccessionID)
                                          )]

print("Number of papers without full text available in NCBI Cloud Service:")
length(not_available)

file_paths = 
c(oa_noncomm_to_get$Key,
  oa_comm_to_get$Key,
  author_manu_to_get$Key)

file_paths <- str_replace_all(file_paths,
                              pattern = "txt",
                              replacement = "xml")

# percentage not available, from all papers
100 * length(not_available) / length(pmids)

```

## Download remaining full texts from NCBI Cloud Service

```{r download_remaining_full_texts, eval = F}

writeLines(
  file_paths,
  here::here("output/fulltexts/aws_locations/selected_paths.txt")
)

system(
  paste(
    "xargs -I {} aws s3 cp",
    "s3://pmc-oa-opendata/{}",
    shQuote(here::here("output/fulltexts/ncbi_cloud/")),
    "--no-sign-request",
    "<",
    shQuote(here::here("output/fulltexts/aws_locations/selected_paths.txt"))
  )
)

```

# Download PDFs using Open Access information from Open Alex

### PMIDs that couldn't be converted to PMCIDs

```{r eval=FALSE}

# old getting dois:
entrez_info <-
entrez_summary(db="pubmed", 
               id=not_convertable_pmids)

dois <-
entrez_info |>
  purrr::map(function(x) {
    
    x$articleids |> 
      filter(idtype == "doi") |> 
      pull(value)
  }
)

```

```{r open_alex_full_text}

library(openalexR)

convert_pmid_df <- fread(here::here("data/europe_pmc/PMID_PMCID_DOI.csv"))

not_convertable_pmids <- converted_ids |> 
                         filter(pmcids == "") |> 
                         pull(PMID)

doi_information <-
convert_pmid_df |>
  filter(PMID %in% not_convertable_pmids)

doi_information |>
  filter(DOI == "")

doi_information$PMID |> unique() |> length()

length(not_convertable_pmids)

# get open alex works for pmids
open_alex_works <- oa_fetch(
  doi = unique(doi_information$DOI),
  entity = "works",
  options = list(select = c("doi", 
                            "open_access"))
)

# no best open access location: 
open_alex_works |> 
  filter(is.na(oa_url)) |>
  nrow()

# pdf link available:
open_alex_works |> 
  filter(grepl("pdf", oa_url)) |>
  nrow()

to_download_pdfs <-
open_alex_works |> 
  filter(grepl(".pdf", oa_url)) |>
  pull(oa_url)

  writeLines(
    to_download_pdfs,
    here::here("output/fulltexts/pdfs/pdf_links_to_download.txt"))

```

```{bash download_pdfs_1, eval = F}

cd output/fulltexts/pdfs

while read -r url; do
  curl -O "$url"
done < pdf_links_to_download.txt

```

### PMCIDs not found in Author Manuscripts or Open Access sections

```{r, eval=FALSE}

doi_information <-
convert_pmid_df |>
  filter(PMCID %in% not_available)

doi_information |>
  filter(DOI == "")

doi_information <-
  doi_information |>
  filter(DOI != "")

# get open alex works for pmcids
open_alex_works <- oa_fetch(
  doi = doi_information$DOI,
  entity = "works",
  options = list(select = c(#"title",
                            "doi", 
                            "open_access"
                            ))
)

# no best open access location: 
open_alex_works |> 
  filter(is.na(oa_url)) |>
  nrow()

# pdf link available:
open_alex_works |> 
  filter(grepl("pdf", oa_url)) |>
  nrow()

to_download_pdfs <-
open_alex_works |> 
  filter(grepl(".pdf", oa_url)) |>
  pull(oa_url)

  writeLines(
    to_download_pdfs,
    here::here("output/fulltexts/pdfs/pdf_links_to_download_pt2.txt"))

```

```{bash download_pdfs_2, eval = F}

cd output/fulltexts/pdfs

while read -r url; do
  curl -O "$url"
done < pdf_links_to_download_pt2.txt

```

# Test: not used - Europe PMC Author Manuscripts

```{bash get_filelists, eval = FALSE}

curl -s https://europepmc.org/ftp/manuscripts/ \
  | grep -o 'author_manuscript_txt[^"]*\.filelist\.txt' \
  | sort -u \
  | while read -r file; do
      curl -O "https://europepmc.org/ftp/manuscripts/$file"
    done

```

```{r europe_pmc_author_manuscripts, eval = F}

all_file_lists <- list.files(here::here("data/epmc"))

author_manu_epmc <- all_file_lists |>
                    purrr::map(function(file_name) {
                      
                      file_path = here::here("data/epmc",
                                             file_name
                                             )
                      
                      df <- fread(file_path)
                      
                      return(df)
                    }
                    ) |>
                    bind_rows()

author_manu_epmc |>
  filter(AccessionID %in% not_available)

author_manu_epmc |>
  filter(PMID %in% not_avaliable_pmids)

author_manu_epmc |>
  filter(PMID %in% pmids)

author_manu_epmc |>
  filter(PMID %in% not_convertable_pmids)

```

# Test: not used - Download with ftp service, where avaliable

```{r download_ftp_service, eval = F}

# not_available
not_convertable_pmids <- converted_ids |> 
                         filter(pmcids == "") |> 
                         pull(PMID)

all_tgz_links = c()

for(article_id in "PMC2613843"){

url <- paste0("https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id=",
              article_id)
  
resp <- GET(url)

xml_data <- xml_child(content(resp), "records")

tgz_link <- xml_find_first(xml_data, 
                           ".//link[@format='tgz']/@href")
tgz_link <- xml_text(tgz_link)

if (is.na(tgz_link)) {
  
  print("No tar.gz link found.")
  
} else {
  
  all_tgz_links <- append(all_tgz_links, 
                          tgz_link)
  
}
}

```
